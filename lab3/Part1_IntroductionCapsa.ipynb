{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SWa-rLfIlTaf"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab3/Part1_IntroductionCapsa.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/aamini/introtodeeplearning/blob/master/lab3/Part1_IntroductionCapsa.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LohleBMlahL"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 MIT Introduction to Deep Learning. All Rights Reserved.\n",
        "# \n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
        "# to Deep Learning must reference:\n",
        "#\n",
        "# Â© MIT Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckzz5Hus-hJB"
      },
      "source": [
        "# Laboratory 3: Debiasing, Uncertainty, and Robustness\n",
        "\n",
        "# Part 1: Introduction to Capsa\n",
        "\n",
        "In this lab, we'll explore different ways to make deep learning models more **robust** and **trustworthy**.\n",
        "\n",
        "To achieve this it is critical to be able to identify and diagnose issues of bias and uncertainty in deep learning models, as we explored in the Facial Detection Lab 2. We need benchmarks that uniformly measure how uncertain a given model is, and we need principled ways of measuring bias and uncertainty. To that end, in this lab, we'll utilize [Capsa](https://github.com/themis-ai/capsa), a risk-estimation wrapping library developed by [Themis AI](https://themisai.io/). Capsa supports the estimation of three different types of ***risk***, defined as measures of how robust and trustworthy our model is. These are:\n",
        "1. **Representation bias**: reflects how likely combinations of features are to appear in a given dataset. Often, certain combinations of features are severely under-represented in datasets, which means models learn them less well and can thus lead to unwanted bias.\n",
        "2. **Data uncertainty**: reflects noise in the data, for example when sensors have noisy measurements, classes in datasets have low separations, and generally when very similar inputs lead to drastically different outputs. Also known as *aleatoric* uncertainty. \n",
        "3. **Model uncertainty**: captures the areas of our underlying data distribution that the model has not yet learned or has difficulty learning. Areas of high model uncertainty can be due to out-of-distribution (OOD) samples or data that is harder to learn. Also known as *epistemic* uncertainty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o02MyoDrnNqP"
      },
      "source": [
        "## CAPSA overview\n",
        "\n",
        "This lab introduces Capsa and its functionalities, to next build automated tools that use Capsa to mitigate the underlying issues of bias and uncertainty.\n",
        "\n",
        "The core idea behind [Capsa](https://themisai.io/capsa/) is that any deep learning model of interest can be ***wrapped*** -- just like wrapping a gift -- to be made ***aware of its own risks***. Risk is captured in representation bias, data uncertainty, and model uncertainty.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/2023/lab3/img/capsa_overview.png)\n",
        "\n",
        "This means that Capsa takes the user's original model as input, and modifies it minimally to create a risk-aware variant while preserving the model's underlying structure and training pipeline. Capsa is a one-line addition to any training workflow in TensorFlow. In this part of the lab, we'll apply Capsa's risk estimation methods to a simple regression problem to further explore the notions of bias and uncertainty. \n",
        "\n",
        "Please refer to [Capsa's documentation](https://themisai.io/capsa/) for additional details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF0uSqk-nwmA"
      },
      "source": [
        "Let's get started by installing the necessary dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NdXF4Reyj6yy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: comet_ml in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (3.35.5)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (4.20.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (5.9.7)\n",
            "Requirement already satisfied: python-box<7.0.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (6.1.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (2.31.0)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (1.39.1)\n",
            "Requirement already satisfied: simplejson in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (3.19.2)\n",
            "Requirement already satisfied: six in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (2.1.0)\n",
            "Requirement already satisfied: websocket-client<1.4.0,>=0.55.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (1.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (1.14.1)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (3.0.3)\n",
            "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from everett[ini]<3.2.0,>=1.0.1; python_version > \"3.5\"->comet_ml) (3.1.0)\n",
            "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (0.21.7)\n",
            "Requirement already satisfied: rich>=13.3.2 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from comet_ml) (13.7.0)\n",
            "Requirement already satisfied: configobj in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from everett[ini]<3.2.0,>=1.0.1; python_version > \"3.5\"->comet_ml) (5.0.8)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from requests>=2.18.4->comet_ml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from requests>=2.18.4->comet_ml) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from requests>=2.18.4->comet_ml) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from rich>=13.3.2->comet_ml) (2.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/alamshorna/6-s191lab2-2-1/335de97019f04a7c9f218e5d78e7f46e\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (604 bytes)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/alamshorna/6-s191lab3-1-1/b4519279411840de93d67b140b03986b\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: mitdeeplearning in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from mitdeeplearning) (1.26.2)\n",
            "Requirement already satisfied: regex in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from mitdeeplearning) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from mitdeeplearning) (4.66.1)\n",
            "Requirement already satisfied: gym in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from mitdeeplearning) (0.26.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from gym->mitdeeplearning) (3.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from gym->mitdeeplearning) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from gym->mitdeeplearning) (7.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/shornaalam/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.8.0->gym->mitdeeplearning) (3.17.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement capsa (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for capsa\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'capsa'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Download and import Capsa\u001b[39;00m\n\u001b[1;32m     22\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install capsa\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcapsa\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'capsa'"
          ]
        }
      ],
      "source": [
        "# Import Tensorflow 2.0\n",
        "# %tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import IPython\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download and import the MIT Introduction to Deep Learning package\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "# Download and import Capsa\n",
        "!pip install capsa\n",
        "import capsa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEcxjKHn8gc"
      },
      "source": [
        "## 1.1 Dataset\n",
        "\n",
        "We will build understanding of bias and uncertainty by training a neural network for a simple 2D regression task: modeling the function $y = x^3$. We will use Capsa to analyze this dataset and the performance of the model. Noise and missing-ness will be injected into the dataset.\n",
        "\n",
        "Let's generate the dataset and visualize it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fH40EhC1j9dH"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Plot the dataset and visualize the train and test datapoints\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m x_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mgen_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# train data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m x_test, y_test \u001b[38;5;241m=\u001b[39m gen_data(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m500\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# test data\u001b[39;00m\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
            "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mgen_data\u001b[0;34m(x_min, x_max, n, train)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_data\u001b[39m(x_min, x_max, n, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m train: \n\u001b[0;32m----> 5\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mtriangular(x_min, \u001b[38;5;241m2\u001b[39m, x_max, size\u001b[38;5;241m=\u001b[39m(n, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m      7\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(x_min, x_max, n)\u001b[38;5;241m.\u001b[39mreshape(n, \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# Get the data for the cubic function, injected with noise and missing-ness\n",
        "# This is just a toy dataset that we can use to test some of the wrappers on\n",
        "def gen_data(x_min, x_max, n, train=True):\n",
        "  if train: \n",
        "    x = np.random.triangular(x_min, 2, x_max, size=(n, 1))\n",
        "  else: \n",
        "    x = np.linspace(x_min, x_max, n).reshape(n, 1)\n",
        "\n",
        "  sigma = 2*np.exp(-(x+1)**2/1) + 0.2 if train else np.zeros_like(x)\n",
        "  y = x**3/6 + np.random.normal(0, sigma).astype(np.float32)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "# Plot the dataset and visualize the train and test datapoints\n",
        "x_train, y_train = gen_data(-4, 4, 2000, train=True) # train data\n",
        "x_test, y_test = gen_data(-6, 6, 500, train=False) # test data\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_test, y_test, c='r', zorder=-1, label='ground truth')\n",
        "plt.scatter(x_train, y_train, s=1.5, label='train data')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz3UxT8vuN95"
      },
      "source": [
        "In the plot above, the blue points are the training data, which will be used as inputs to train the neural network model. The red line is the ground truth data, which will be used to evaluate the performance of the model.\n",
        "\n",
        "#### **TODO: Inspecting the 2D regression dataset**\n",
        "\n",
        " Write short (~1 sentence) answers to the questions below to complete the `TODO`s:\n",
        "\n",
        "1. What are your observations about where the train data and test data lie relative to each other?\n",
        "2. What, if any, areas do you expect to have high/low aleatoric (data) uncertainty?\n",
        "3. What, if any, areas do you expect to have high/low epistemic (model) uncertainty?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXMOYRHnv8tF"
      },
      "source": [
        "## 1.2 Regression on cubic dataset\n",
        "\n",
        "Next we will define a small dense neural network model that can predict `y` given `x`: this is a classical regression task! We will build the model and use the [`model.fit()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) function to train the model -- normally, without any risk-awareness -- using the train dataset that we visualized above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p1XwfZVuB68"
      },
      "outputs": [],
      "source": [
        "### Define and train a dense NN model for the regression task###\n",
        "\n",
        "'''Function to define a small dense NN'''\n",
        "def create_dense_NN():\n",
        "  return tf.keras.Sequential(\n",
        "          [\n",
        "              tf.keras.Input(shape=(1,)),\n",
        "              tf.keras.layers.Dense(32, \"relu\"),\n",
        "              tf.keras.layers.Dense(32, \"relu\"),\n",
        "              tf.keras.layers.Dense(32, \"relu\"),\n",
        "              tf.keras.layers.Dense(1),\n",
        "          ]\n",
        "  )\n",
        "\n",
        "dense_NN = create_dense_NN()\n",
        "\n",
        "# Build the model for regression, defining the loss function and optimizer\n",
        "dense_NN.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=5e-3),\n",
        "  loss=tf.keras.losses.MeanSquaredError(), # MSE loss for the regression task\n",
        ")\n",
        "\n",
        "# Train the model for 30 epochs using model.fit().\n",
        "loss_history = dense_NN.fit(x_train, y_train, epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovwYBUG3wTDv"
      },
      "source": [
        "Now, we are ready to evaluate our neural network. We use the test data to assess performance on the regression task, and visualize the predicted values against the true values.\n",
        "\n",
        "Given your observation of the data in the previous plot, where do you expect the model to perform well? Let's test the model and see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb-EklZywR4D"
      },
      "outputs": [],
      "source": [
        "# Pass the test data through the network and predict the y values\n",
        "y_predicted = dense_NN.predict(x_test)\n",
        "\n",
        "# Visualize the true (x, y) pairs for the test data vs. the predicted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x_train, y_train, s=1.5, label='train data')\n",
        "plt.plot(x_test, y_test, c='r', zorder=-1, label='ground truth')\n",
        "plt.plot(x_test, y_predicted, c='b', zorder=0, label='predicted')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vktjwfu0ReH"
      },
      "source": [
        "\n",
        "#### **TODO: Analyzing the performance of standard regression model**\n",
        "\n",
        "Write short (~1 sentence) answers to the questions below to complete the `TODO`s:\n",
        "\n",
        "1. Where does the model perform well?\n",
        "2. Where does the model perform poorly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MzvM48JyZMO"
      },
      "source": [
        "## 1.3 Evaluating bias\n",
        "\n",
        "Now that we've seen what the predictions from this model look like, we will identify and quantify bias and uncertainty in this problem. We first consider bias.\n",
        "\n",
        "Recall that *representation bias* reflects how likely combinations of features are to appear in a given dataset. Capsa calculates how likely combinations of features are by using a histogram estimation approach: the `capsa.HistogramWrapper`. For low-dimensional data, the `capsa.HistogramWrapper` bins the input directly into discrete categories and measures the density. More details of the `HistogramWrapper` and how it can be used are [available here](https://themisai.io/capsa/api_documentation/HistogramWrapper.html).\n",
        "\n",
        "We start by taking our `dense_NN` and wrapping it with the `capsa.HistogramWrapper`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVv-knsCwOp9"
      },
      "outputs": [],
      "source": [
        "### Wrap the dense network for bias estimation ###\n",
        "\n",
        "standard_dense_NN = create_dense_NN()\n",
        "bias_wrapped_dense_NN = capsa.HistogramWrapper(\n",
        "    standard_dense_NN, # the original model\n",
        "    num_bins=20,\n",
        "    queue_size=2000, # how many samples to track\n",
        "    target_hidden_layer=False # for low-dimensional data (like this dataset), we can estimate biases directly from data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFHO7LKcz8uP"
      },
      "source": [
        "Now that we've wrapped the classifier, let's re-train it to update the bias estimates as we train. We can use the exact same training pipeline, using `compile` to build the model and `model.fit()` to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkyD3rsqy2ff"
      },
      "outputs": [],
      "source": [
        "### Compile and train the wrapped model! ###\n",
        "\n",
        "# Build the model for regression, defining the loss function and optimizer\n",
        "bias_wrapped_dense_NN.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=2e-3),\n",
        "  loss=tf.keras.losses.MeanSquaredError(), # MSE loss for the regression task\n",
        ")\n",
        "\n",
        "# Train the wrapped model for 30 epochs.\n",
        "loss_history_bias_wrap = bias_wrapped_dense_NN.fit(x_train, y_train, epochs=30)\n",
        "\n",
        "print(\"Done training model with Bias Wrapper!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6iVeeqq0f_H"
      },
      "source": [
        "We can now use our wrapped model to assess the bias for a given test input. With the wrapping capability, Capsa neatly allows us to output a *bias score* along with the predicted target value. This bias score reflects the density of data surrounding an input point -- the higher the score, the greater the data representation and density. The wrapped, risk-aware model outputs the predicted target and bias score after it is called!\n",
        "\n",
        "Let's see how it is done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ17eCbP0YM4"
      },
      "outputs": [],
      "source": [
        "### Generate and visualize bias scores for data in test set ###\n",
        "\n",
        "# Call the risk-aware model to generate scores\n",
        "predictions, bias = bias_wrapped_dense_NN(x_test)\n",
        "\n",
        "# Visualize the relationship between the input data x and the bias\n",
        "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
        "ax[0].plot(x_test, bias, label='bias')\n",
        "ax[0].set_ylabel('Estimated Bias')\n",
        "ax[0].legend()\n",
        "\n",
        "# Let's compare against the ground truth density distribution\n",
        "#   should roughly align with our estimated bias in this toy example\n",
        "ax[1].hist(x_train, 50, label='ground truth')\n",
        "ax[1].set_xlim(-6, 6)\n",
        "ax[1].set_ylabel('True Density')\n",
        "ax[1].legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpDMT_1FERQE"
      },
      "source": [
        "#### **TODO: Evaluating bias with wrapped regression model**\n",
        "\n",
        "Write short (~1 sentence) answers to the questions below to complete the `TODO`s:\n",
        "\n",
        "1. How does the bias score relate to the train/test data density from the first plot?\n",
        "2. What is one limitation of the Histogram approach that simply bins the data based on frequency?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvS8xR_q27Ec"
      },
      "source": [
        "# 1.4 Estimating data uncertainty\n",
        "\n",
        "Next we turn our attention to uncertainty, first focusing on the uncertainty in the data -- the aleatoric uncertainty.\n",
        "\n",
        "As introduced in Lecture 5 on Robust & Trustworthy Deep Learning, in regression we can estimate aleatoric uncertainty by training the model to predict both a target value and a variance for every input. Because we estimate both a mean and variance for every input, this method is called Mean Variance Estimation (MVE). MVE involves modifying the output layer to predict both the mean and variance, and changing the loss to reflect the prediction likelihood.\n",
        "\n",
        "Capsa automatically implements these changes for us: we can wrap a given model using `capsa.MVEWrapper` to use MVE to estimate aleatoric uncertainty. All we have to do is define the model and the loss function to evaluate its predictions! More details of the `MVEWrapper` and how it can be used are [available here](https://themisai.io/capsa/api_documentation/MVEWrapper.html).\n",
        "\n",
        "Let's take our standard network, wrap it with `capsa.MVEWrapper`, build the wrapped model, and then train it for the regression task. Finally, we evaluate performance of the resulting model by quantifying the aleatoric uncertainty across the data space: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxmm-2sd3G9u"
      },
      "outputs": [],
      "source": [
        "### Estimating data uncertainty with Capsa wrapping ###\n",
        "\n",
        "standard_dense_NN = create_dense_NN()\n",
        "# Wrap the dense network for aleatoric uncertainty estimation\n",
        "mve_wrapped_NN = capsa.MVEWrapper(standard_dense_NN)\n",
        "\n",
        "# Build the model for regression, defining the loss function and optimizer\n",
        "mve_wrapped_NN.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
        "  loss=tf.keras.losses.MeanSquaredError(), # MSE loss for the regression task\n",
        ")\n",
        "\n",
        "# Train the wrapped model for 30 epochs.\n",
        "loss_history_mve_wrap = mve_wrapped_NN.fit(x_train, y_train, epochs=30)\n",
        "\n",
        "# Call the uncertainty-aware model to generate outputs for the test data\n",
        "x_test_clipped = np.clip(x_test, x_train.min(), x_train.max())\n",
        "prediction = mve_wrapped_NN(x_test_clipped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT2Rx8JCg3NR"
      },
      "outputs": [],
      "source": [
        "# Capsa makes the aleatoric uncertainty an attribute of the prediction!\n",
        "pred = np.array(prediction.y_hat).flatten()\n",
        "unc = np.sqrt(prediction.aleatoric).flatten() # out.aleatoric is the predicted variance\n",
        "\n",
        "# Visualize the aleatoric uncertainty across the data space\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x_train, y_train, s=1.5, label='train data')\n",
        "plt.plot(x_test, y_test, c='r', zorder=-1, label='ground truth')\n",
        "plt.fill_between(x_test_clipped.flatten(), pred-2*unc, pred+2*unc, \n",
        "                 color='b', alpha=0.2, label='aleatoric')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFeArgRX9U9s"
      },
      "source": [
        "#### **TODO: Estimating aleatoric uncertainty**\n",
        "\n",
        "Write short (~1 sentence) answers to the questions below to complete the `TODO`s:\n",
        "\n",
        "1. For what values of $x$ is the aleatoric uncertainty high or increasing suddenly?\n",
        "2. How does your answer in (1) relate to how the $x$ values are distributed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FC5WPRT5lAb"
      },
      "source": [
        "# 1.5 Estimating model uncertainty\n",
        "\n",
        "Finally, we use Capsa for estimating the uncertainty underlying the model predictions -- the epistemic uncertainty. In this example, we'll use ensembles, which essentially copy the model `N` times and average predictions across all runs for a more robust prediction, and also calculate the variance of the `N` runs to estimate the uncertainty.\n",
        "\n",
        "Capsa provides a neat wrapper, `capsa.EnsembleWrapper`, to make an ensemble from an input model. Just like with aleatoric estimation, we can take our standard dense network model, wrap it with `capsa.EnsembleWrapper`, build the wrapped model, and then train it for the regression task. More details of the `EnsembleWrapper` and how it can be used are [available here](https://themisai.io/capsa/api_documentation/EnsembleWrapper.html).\n",
        "\n",
        "Finally, we evaluate the resulting model by quantifying the epistemic uncertainty on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuRlhq2c5Fob"
      },
      "outputs": [],
      "source": [
        "### Estimating model uncertainty with Capsa wrapping ###\n",
        "\n",
        "standard_dense_NN = create_dense_NN()\n",
        "# Wrap the dense network for epistemic uncertainty estimation with an Ensemble\n",
        "ensemble_NN = capsa.EnsembleWrapper(standard_dense_NN)\n",
        "\n",
        "# Build the model for regression, defining the loss function and optimizer\n",
        "ensemble_NN.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=3e-3),\n",
        "  loss=tf.keras.losses.MeanSquaredError(), # MSE loss for the regression task\n",
        ")\n",
        "\n",
        "# Train the wrapped model for 30 epochs.\n",
        "loss_history_ensemble = ensemble_NN.fit(x_train, y_train, epochs=30)\n",
        "\n",
        "# Call the uncertainty-aware model to generate outputs for the test data\n",
        "prediction = ensemble_NN(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eauNoKDOj_ZT"
      },
      "outputs": [],
      "source": [
        "# Capsa makes the epistemic uncertainty an attribute of the prediction!\n",
        "pred = np.array(prediction.y_hat).flatten()\n",
        "unc = np.array(prediction.epistemic).flatten()\n",
        "\n",
        "# Visualize the aleatoric uncertainty across the data space\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x_train, y_train, s=1.5, label='train data')\n",
        "plt.plot(x_test, y_test, c='r', zorder=-1, label='ground truth')\n",
        "plt.fill_between(x_test.flatten(), pred-20*unc, pred+20*unc, color='b', alpha=0.2, label='epistemic')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4LMn2tLPBdg"
      },
      "source": [
        "#### **TODO: Estimating epistemic uncertainty**\n",
        "\n",
        "Write short (~1 sentence) answers to the questions below to complete the `TODO`s:\n",
        "\n",
        "1. For what values of $x$ is the epistemic uncertainty high or increasing suddenly?\n",
        "2. How does your answer in (1) relate to how the $x$ values are distributed (refer back to original plot)? Think about both the train and test data.\n",
        "3. How could you reduce the epistemic uncertainty in regions where it is high?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkpvkOL06jRd"
      },
      "source": [
        "# 1.6 Conclusion\n",
        "\n",
        "You've just analyzed the bias, aleatoric uncertainty, and epistemic uncertainty for your first risk-aware model! This is a task that data scientists do constantly to determine methods of improving their models and datasets.\n",
        "\n",
        "In the next part of the lab, you'll continue to build off of these concepts to study them in the context of facial detection systems: not only diagnosing issues of bias and uncertainty, but also developing solutions to *mitigate* these risks.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/2023/lab3/img/solutions_toy.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIpfPcpjlsKK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
