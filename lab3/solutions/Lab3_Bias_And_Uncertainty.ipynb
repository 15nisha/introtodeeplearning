{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgYKebt871EK"
      },
      "source": [
        "# Laboratory 3: Detecting and mitigating bias and uncertainty in Facial Detection Systems\n",
        "In this lab, we'll continue to explore how to mitigate algorithmic bias in facial recognition systems. In addition, we'll explore the notion of *uncertainty* in datasets, and learn how to reduce both data-based and model-based uncertainty.\n",
        "\n",
        "As we've seen in lecture 5, bias and uncertainty underlie many common issues with machine learning models today, and these are not just limited to classification tasks. Automatically detecting and mitigating uncertainty is crucial to deploying fair and safe models. \n",
        "\n",
        "In this lab, we'll be using [CAPSA](https://github.com/themis-ai/capsa/), a software package developed by [Themis AI](https://themisai.io/), which automatically *wraps* models to make them risk-aware and plugs into training workflows. We'll explore how we can use CAPSA to diagnose uncertainties, and then develop methods for automatically mitigating them.\n",
        "\n",
        "\n",
        "Run the next code block for a short video from Google that explores how and why it's important to consider bias when thinking about machine learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ll7uZ8q72hm",
        "outputId": "56b3117b-e344-481b-a9fc-2798b76d7a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'capsa-intro-deep-learning' already exists and is not an empty directory.\n",
            "Already on 'HistogramVAEWrapper'\n",
            "Your branch is up to date with 'origin/HistogramVAEWrapper'.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/slolla/capsa-intro-deep-learning.git\n",
        "!cd capsa-intro-deep-learning/ && git checkout HistogramVAEWrapper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JTRoM7E71EU"
      },
      "source": [
        "Let's get started by installing the relevant dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjAn-WZK9lOv",
        "outputId": "35e24600-85b4-4320-c436-061856e56861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/capsa-intro-deep-learning\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/capsa-intro-deep-learning\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: capsa\n",
            "  Attempting uninstall: capsa\n",
            "    Found existing installation: capsa 0.1.2\n",
            "    Can't uninstall 'capsa'. No files were found to uninstall.\n",
            "  Running setup.py develop for capsa\n",
            "Successfully installed capsa-0.1.2\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd capsa-intro-deep-learning/\n",
        "%pip install -e .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pzGVPrh-4LQ",
        "outputId": "f4588f12-d290-4746-d819-501a0e3ba390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'introtodeeplearning' already exists and is not an empty directory.\n",
            "Already on '2023'\n",
            "Your branch is up to date with 'origin/2023'.\n",
            "/content/introtodeeplearning\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/introtodeeplearning\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mitdeeplearning==0.3.0) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from mitdeeplearning==0.3.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mitdeeplearning==0.3.0) (4.64.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from mitdeeplearning==0.3.0) (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym->mitdeeplearning==0.3.0) (5.2.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->mitdeeplearning==0.3.0) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->mitdeeplearning==0.3.0) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym->mitdeeplearning==0.3.0) (3.11.0)\n",
            "Installing collected packages: mitdeeplearning\n",
            "  Attempting uninstall: mitdeeplearning\n",
            "    Found existing installation: mitdeeplearning 0.3.0\n",
            "    Can't uninstall 'mitdeeplearning'. No files were found to uninstall.\n",
            "  Running setup.py develop for mitdeeplearning\n",
            "Successfully installed mitdeeplearning-0.3.0\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aamini/introtodeeplearning.git\n",
        "!cd introtodeeplearning/ && git checkout 2023\n",
        "%cd introtodeeplearning/\n",
        "%pip install -e .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2PdAhs1371EU"
      },
      "outputs": [],
      "source": [
        "# Import Tensorflow 2.0\n",
        "#%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import IPython\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from capsa import *\n",
        "# Download and import the MIT 6.S191 package\n",
        "from mitdeeplearning import lab3 \n",
        "# Download and import capsa\n",
        "#!pip install capsa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VKVqLb371EV"
      },
      "source": [
        "## 3.1 Datasets\n",
        "\n",
        "We'll be using the same datasets from lab 2 in this lab. Note that in this dataset, we've intentionally perturbed some of the samples in some ways (it's up to you to figure out how!) that are not necessarily present in the actual dataset. \n",
        "\n",
        "1.   **Positive training data**: [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). A large-scale (over 200K images) of celebrity faces.   \n",
        "2.   **Negative training data**: [ImageNet](http://www.image-net.org/). Many images across many different categories. We'll take negative examples from a variety of non-human categories. \n",
        "[Fitzpatrick Scale](https://en.wikipedia.org/wiki/Fitzpatrick_scale) skin type classification system, with each image labeled as \"Lighter'' or \"Darker''.\n",
        "\n",
        "Like before, let's begin by importing these datasets. We've written a class that does a bit of data pre-processing to import the training data in a usable format.\n",
        "\n",
        "Also note that in this lab, we'll be using a much larger test dataset for evaluation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIA6EA1D71EW",
        "outputId": "df98738c-00d5-4987-bd58-938dd17c8ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Opening /root/.keras/datasets/train_face_2023_v2.h5\n",
            "Loading data into memory...\n",
            "Opening /root/.keras/datasets/train_face_2023_v2.h5\n",
            "Loading data into memory...\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Get the training data: both images from CelebA and ImageNet\n",
        "path_to_training_data = tf.keras.utils.get_file('train_face_perturbed_small.h5', 'https://www.dropbox.com/s/tbra3danrk5x8h5/train_face_2023_perturbed_small.h5?dl=1')\n",
        "# Instantiate a DatasetLoader using the downloaded dataset\n",
        "train_loader = lab3.DatasetLoader(path_to_training_data, training=True, batch_size= batch_size)\n",
        "test_loader = lab3.DatasetLoader(path_to_training_data, training=False, batch_size = batch_size)\n",
        "train_imgs = train_loader.get_all_faces()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cREmhMWJ71EX"
      },
      "source": [
        "### Recap: Thinking about bias and uncertainty\n",
        "\n",
        "Remember that we'll be training our facial detection classifiers on the large, well-curated CelebA dataset (and ImageNet), and then evaluating their accuracy by testing them on an independent test dataset. Our goal is to build a model that trains on CelebA *and* achieves high classification accuracy on the the test dataset across all demographics, and to thus show that this model does not suffer from any hidden bias. \n",
        "\n",
        "In addition to thinking about bias, we want to detect areas of high *aleatoric* uncertainty in the dataset, which is defined as data noise: in the context of facial detection, this means that we may have very similar inputs with different labels-- think about the scenario where one face is labeled correctly as a positive, and another face is labeled incorrectly as a negative. \n",
        "\n",
        "Finally, we want to look at samples with high *epistemic*, or predictive, uncertainty. These may be samples that are anomalous or out of distribution, samples that contain adversarial noise, or samples that are \"harder\" to learn in some way. Importantly, epistemic uncertainty is not the same as bias! We may have well-represented samples that still have high epistemic uncertainty. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NhotGiT71EY"
      },
      "source": [
        "# 3.2 Bias\n",
        "\n",
        "In the previous lab, we used a variational autoencoder (VAE) to automatically learn the latent structure of our database, and we developed a scoring mechanism for samples to determine their bias. In this lab, we'll show that we can use CAPSA to do the same thing in one line! Then, our goal will be to continue our implementation of the DB-VAE and use the latent variables learned via a VAE to adaptively re-sample the CelebA data during training. Specifically, we will alter the probability that a given image is used during training based on how often its latent features appear in the dataset. So, faces with rarer features (like dark skin, sunglasses, or hats) should become more likely to be sampled during training, while the sampling probability for faces with features that are over-represented in the training dataset should decrease (relative to uniform random sampling across the training data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niy4he0m71EZ"
      },
      "source": [
        "Just like the last lab, let's define a standard classifier that we'll use as the base encoder of our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5hQb75Vm71EZ"
      },
      "outputs": [],
      "source": [
        "### Define the CNN model ###\n",
        "\n",
        "n_filters = 12 # base number of convolutional filters\n",
        "\n",
        "'''Function to define a standard CNN model'''\n",
        "def make_standard_classifier(n_outputs=1):\n",
        "  Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
        "  BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "  Flatten = tf.keras.layers.Flatten\n",
        "  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "\n",
        "  model = tf.keras.Sequential([ \n",
        "    tf.keras.Input(shape=(64,64, 3)),\n",
        "    Conv2D(filters=1*n_filters, kernel_size=5,  strides=2),\n",
        "    BatchNormalization(),\n",
        "    \n",
        "    Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512),\n",
        "    Dense(n_outputs, activation=None),\n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgTG6buf71Ea"
      },
      "source": [
        "Let's use CAPSA's `HistogramVAEWrapper` to analyze the latent space distribution as we did previously. The `HistogramVAEWrapper` constructs a histogram with `num_bins` bins across every dimension of the latent space, and then calculates the joint probability of every sample according to the histograms. The samples with the lowest joint probability have the lowest bias, and we want to oversample these. Conversely, we want to undersample the areas of the dataset with the highest bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FivHOdGE71Ea"
      },
      "source": [
        "The `HistogramVAEWrapper` class takes in a number of arguments: namely, the number of bins we want to discretize our distribution into, the number of samples we want to track at any given point, and whether we're using the output of a hidden layer (good for higher-dimensional data) or the input data itself (good for lower-dimensional data). Since this is a variational autoencoder, we need to also pass in a decoder. Let's define the same decoder as the previous lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zTat3K8E71Eb"
      },
      "outputs": [],
      "source": [
        "def make_face_decoder_network(n_filters=12):\n",
        "  # Functionally define the different layer types we will use\n",
        "  Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
        "  BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "  Flatten = tf.keras.layers.Flatten\n",
        "  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "  Reshape = tf.keras.layers.Reshape\n",
        "\n",
        "  # Build the decoder network using the Sequential API\n",
        "  decoder = tf.keras.Sequential([\n",
        "    # Transform to pre-convolutional generation\n",
        "    Dense(units=4*4*6*n_filters),  # 4x4 feature maps (with 6N occurances)\n",
        "    Reshape(target_shape=(4, 4, 6*n_filters)),\n",
        "\n",
        "    # Upscaling convolutions (inverse of encoder)\n",
        "    Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),\n",
        "    Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),\n",
        "    Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),\n",
        "    Conv2DTranspose(filters=3, kernel_size=5,  strides=2),\n",
        "  ])\n",
        "\n",
        "  return decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i4JmvmMA71Ec"
      },
      "outputs": [],
      "source": [
        "standard_classifier = make_standard_classifier()\n",
        "wrapped_classifier = HistogramVAEWrapper(standard_classifier, num_bins=5, queue_size=20000, latent_dim = 100, decoder=make_face_decoder_network())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "valYm5LH71Ec"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A527wdyV71Ec"
      },
      "source": [
        "Now, let's train the wrapped classifier! As we did in the previous lab, in addition to updating the weights of the model, the wrapped classifier also tracks feature distributions. We can use the joint probabilities of these feature distributions to determine the bias of a given sample in this dataset. We'll make use of the `Model.fit` API here, but note that we can achieve the same behavior with a custom training loop as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmshVdLM71Ed",
        "outputId": "48155283-4767-46e7-e84b-dfd3ac8c1917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_1/kernel:0', 'dense_1/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['dense_1/kernel:0', 'dense_1/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 102/2404 [>.............................] - ETA: 5:58 - vae_compiled_loss: 0.8147 - vae_compiled_binary_accuracy: 0.4792 - vae_wrapper_loss: 3385.2124"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-5\n",
        "\n",
        "# compile model using desired optimizers and losses\n",
        "wrapped_classifier.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
        "    run_eagerly=True\n",
        ")\n",
        "\n",
        "# fit the model to our training data\n",
        "history = wrapped_classifier.fit(\n",
        "        train_loader,\n",
        "        epochs=6,\n",
        "        batch_size=batch_size,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzFGcrhv71Ed"
      },
      "source": [
        "Let's see what the bias looks like on our test dataset! Note that in this lab, we're using a much larger test dataset than the one in Lab 2. By calling the `wrapped_classifier` on our test set, we can automatically generate the same bias scores that we manually calculated in the last lab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dCqvPFH71Ed"
      },
      "outputs": [],
      "source": [
        "test_imgs = test_loader.get_all_faces() # Get all faces from the testing dataset\n",
        "predictions, _, bias = wrapped_classifier.predict(test_imgs) # use CAPSA-wrapped classifier to obtain estimates for bias and the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt7_FlRW71Ee"
      },
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtc0kjE471Ee"
      },
      "source": [
        "Now, we have an estimate for the bias score! Let's visualize what the samples with the highest bias and those with the lowest bias look like. Before you run the next code block, which faces would you expect to be underrepresented in the dataset? Which ones do you think will be overrepresented?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYMRqq5E71Ee"
      },
      "outputs": [],
      "source": [
        "indices = np.argsort(bias, axis=None) \n",
        "sorted_images = test_imgs[indices] # sort images from lowest to highest bias\n",
        "sorted_biases = bias[indices]\n",
        "sorted_preds = predictions[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAYaFUj-71Ee"
      },
      "outputs": [],
      "source": [
        "lab3.plot_k(sorted_images[:20]) # These are the samples with the lowest representation (least bias) in our test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnbR3qAF71Ef"
      },
      "outputs": [],
      "source": [
        "lab3.plot_k(sorted_images[-20:]) # These are the samples with the highest representation (most bias) in our test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JYmGMJF71Ef"
      },
      "source": [
        "Now, we'll spend some time looking at the bias by *percentile* in our dataset. First, let's plot the accuracy as the bias increases. Remember that we use bias to quantify the level of representation in our dataset, so increasing bias means increasing representation. How do you expect the accuracy to change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfzlOhWi71Ef"
      },
      "outputs": [],
      "source": [
        "averaged_imgs = lab3.plot_accuracy_vs_risk(sorted_images, sorted_biases, sorted_preds, \"Bias vs. Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8ERzg2-71Ef"
      },
      "source": [
        "Now, for a super interesting visualization, let's look at the *percentiles* of bias: what does the average face in the 10th percentile of bias look like? What about the 90th percentile? What changes across these faces?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cd590UP71Ef"
      },
      "outputs": [],
      "source": [
        "lab3.plot_percentile(averaged_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRNV-3SU71Eg"
      },
      "source": [
        "Now that we know what the bias in our dataset looks like, let's adaptively resample from our dataset! Since we can calculate this score on-the-fly *during training*, we can adjust the probability of samples being chosen. But first, let's also take a look at the *epistemic* uncertainty of this dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww5lx7ue71Eg"
      },
      "source": [
        "# 3.3 Epistemic Uncertainty\n",
        "\n",
        "Recall from lecture that *epistemic* uncertainty, or a model's uncertainty in its prediction, can arise from out of distribution data, or samples that are harder to learn. This does not necessarily correlate with bias! Imagine the scenario of training an object detector for self-driving cars: even if the model is presented with many cluttered scenes, these samples still may be harder to learn than scenes with very few objects in them. In this part of the lab, we'll analyze the epistemic uncertainty of the VAE that we've trained on this dataset. \n",
        "\n",
        "From lecture 6, we saw that most methods of estimating epistemic uncertainty are *sampling-based*, but we can also use *reconstruction-based* methods. If a model is unable to provide a good reconstruction for a given data point, it has not learned that area of the underlying data distribution well, and therefore has high epistemic uncertainty. \n",
        "\n",
        "Since we've already used a VAE to calculate the histograms for bias quantification, we can use the same VAE to shed insight into epistemic uncertainty! CAPSA helps us do exactly that: when call the model, we get the bias, reconstruction loss, and prediction for every sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwGPvdZm71Eg"
      },
      "outputs": [],
      "source": [
        "predictions, reconstruction_loss, bias = wrapped_classifier.predict(test_imgs) # note that we're estimating both bias and uncertainty in a single shot!\n",
        "\n",
        "epistemic_indices = np.argsort(reconstruction_loss, axis=None) \n",
        "epistemic_images = test_imgs[epistemic_indices] # sort images by reconstruction loss this time!\n",
        "sorted_epistemic = reconstruction_loss[epistemic_indices]\n",
        "sorted_epistemic_preds = predictions[epistemic_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB8Iqrfb71Eg"
      },
      "outputs": [],
      "source": [
        "lab3.plot_k(epistemic_images[:20]) # samples with the LEAST epistemic uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miu5h2Pc71Eh"
      },
      "outputs": [],
      "source": [
        "lab3.plot_k(epistemic_images[-20:]) # samples with the MOST epistemic uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dA8EyX71Eh"
      },
      "source": [
        "Let's run the same analysis: check how the accuracy varies with epistemic uncertainty!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzQwvSvA71Eh"
      },
      "outputs": [],
      "source": [
        "_ = lab3.plot_accuracy_vs_risk(epistemic_images, sorted_epistemic, sorted_epistemic_preds, \"Epistemic Uncertainty vs. Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyn0IE6x71Eh"
      },
      "source": [
        "How do these compare to the bias plots? Was this expected or unexpected?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbwRbesM71Eh"
      },
      "source": [
        "# 3.4 Resampling based on risk metrics\n",
        "\n",
        "Finally, let's use both the bias score and the reconstruction loss to adaptively resample from our dataset. Since we can calculate this score on-the-fly *during training*, we can adjust the probability of samples being chosen. \n",
        "\n",
        "Note that we want to debias and amplify only the *positive* samples in the dataset, so we're going to only adjust probabilities and calculate scores for these samples. \n",
        "\n",
        "We want to *amplify*, or increase the probability of sampling, of images with high epistemic uncertainty, since these data points come from areas of the latent distribution that the model hasn't learned very well yet. We also want to amplify images with very low representation bias, since otherwise, the model won't see enough of these samples during training. Let's define two functions below to do this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRL5nUBs71Ei"
      },
      "source": [
        "First, let's do this for the bias. We have a smoothing parameter `alpha` that we can tune: as `alpha` increases, the probabilities will tend towards a uniform distribution, and as `alpha` decreases, the probabilities will correlate more directly with the bias. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wR2bMw571Ei"
      },
      "outputs": [],
      "source": [
        "def score_to_probability_bias(score, alpha):\n",
        "    score = score + alpha\n",
        "    probabilities = 1/score\n",
        "    probabilities = probabilities/sum(probabilities)\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUs-0O_v71Ei"
      },
      "source": [
        "Let's now define a similar function for the epistemic probabilities: note that in this case, we want high epistemic uncertainty to correlate with a higher probability!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLWGKvc971Ei"
      },
      "outputs": [],
      "source": [
        "def score_to_probability_epistemic(score, beta):\n",
        "    score = score + beta\n",
        "    probabilities = score/sum(score)\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meZxtxFS71Ei"
      },
      "source": [
        "Now, let's redefine and re-train our debiasing model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q99jG0dB71Ei"
      },
      "outputs": [],
      "source": [
        "standard_classifier = make_standard_classifier()\n",
        "dbvae = HistogramVAEWrapper(standard_classifier, latent_dim=100, num_bins=5, queue_size=2000, decoder=make_face_decoder_network())\n",
        "dbvae.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            metrics=[tf.keras.metrics.BinaryAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odFu4u7i71Ei"
      },
      "outputs": [],
      "source": [
        "# The training loop -- outer loop iterates over the number of epochs\n",
        "for i in range(6):\n",
        "\n",
        "  print(\"Starting epoch {}/{}\".format(i+1, 6))\n",
        "  \n",
        "  # get a batch of training data and compute the training step\n",
        "  for step, data in enumerate(train_loader):\n",
        "    metrics = dbvae.train_step(data)\n",
        "    if step % 100 == 0:\n",
        "        print(step)\n",
        "  _, recon_loss, bias_scores = dbvae(train_imgs)\n",
        "  recon_loss = np.squeeze(recon_loss)\n",
        "\n",
        "  # Recompute data sampling proabilities\n",
        "  p_faces = score_to_probability_bias(bias_scores.numpy(), 1e-7)\n",
        "  p_recon = score_to_probability_epistemic(recon_loss, 1e-7)\n",
        "  p_final = (p_faces + p_recon)/2\n",
        "  p_final /= sum(p_final)\n",
        "  \n",
        "  train_loader.p_pos = p_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwXrAeBo71Ej"
      },
      "source": [
        "Now, we should have a debiased model that also mitigates some forms of uncertainty! Let's see how well our model does:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXiB-DMH71Ej"
      },
      "source": [
        "# 3.5 Evaluation\n",
        "\n",
        "Let's run the same analyses as before, and plot the accuracy vs. the bias and accuracy vs. epistemic uncertainty. We want the model to do better on less biased and more uncertain samples than it did previously\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaMnwd_w71Ej"
      },
      "outputs": [],
      "source": [
        "predictions, reconstruction_loss, bias = dbvae.predict(test_imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCXVIsaJ71Ej"
      },
      "outputs": [],
      "source": [
        "indices = np.argsort(bias, axis=None)\n",
        "bias_images = test_imgs[indices]\n",
        "sorted_bias = bias[indices]\n",
        "sorted_bias_preds = predictions[indices]\n",
        "_ = lab3.plot_accuracy_vs_risk(bias_images, sorted_bias, sorted_bias_preds, \"Bias vs. Accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6p2j_xa71Ej"
      },
      "outputs": [],
      "source": [
        "indices = np.argsort(reconstruction_loss, axis=None)\n",
        "epistemic_images = test_imgs[indices]\n",
        "sorted_epistemic = bias[indices]\n",
        "sorted_epistemic_preds = predictions[indices]\n",
        "_ = lab3.plot_accuracy_vs_risk(epistemic_images, sorted_epistemic, sorted_epistemic_preds, \"Epistemic Uncertainty vs. Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1cEEnII71Ej"
      },
      "source": [
        "# 3.6 Conclusion\n",
        "\n",
        "We encourage you to think about and maybe even address some questions raised by the approach and results outlined here:\n",
        "\n",
        "* We did not analyze the *aleatoric* uncertainty of the above dataset. Try to develop a similar approach (assigning probabilities based on aleatoric uncertainty) and incorporate this as well! You may find some surprising results :)\n",
        "\n",
        "* How can the performance of the classifier above be improved even further? We purposely did not optimize hyperparameters to leave this up to you!\n",
        "\n",
        "* How can you use other methods of uncertainty in CAPSA to strengthen your uncertainty estimates?\n",
        "\n",
        "* In which applications (either related to facial detection or not!) would debiasing in this way be desired? Are there applications where you may not want to debias your model?\n",
        "\n",
        "* Try to optimize your model to achieve improved performance. MIT students and affiliates will be eligible for prizes during the IAP offering. To enter the competition, MIT students and affiliates should upload the following to the course Canvas:\n",
        "\n",
        "* Jupyter notebook with the code you used to generate your results;\n",
        "copy of the line plots from section 3.5 showing the performance of your model;\n",
        "* a description and/or diagram of the architecture and hyperparameters you used -- if there are any additional or interesting modifications you made to the template code, please include these in your description;\n",
        "* discussion of why these modifications helped improve performance.\n",
        "\n",
        "Hopefully this lab has shed some light on a few concepts, from vision based tasks, to VAEs, to algorithmic bias. We like to think it has, but we're biased ;)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
